# RStudio Exercise 2 - Regression and model validation

*In this exercise I have been working on a data set about students learning motivation. In preparation I transformed the raw data into a data set suitable for analysis. That process is called data wrangling. After that I was analysing the data statistically in order to investigate the relationship between certain variables.*


## 2.1.Reading students2014 data into R and exploring its dimensions and structure.

```{r}
learning2014 <- read.csv("/Users/eva-mariaroth/Documents/GitHub/IODS-project/learning2014.csv", header = TRUE)
```
```{r dim}
dim(learning2014)
```

```{r str}
str(learning2014)
```
```{r head}
head(learning2014)
```
The dataset consists of 166 observations of 7 variables (columns) related to the learning behaviour of students. The variables are:   
**Gender** : character values, M, F, for male and female  
**Age** : age of the students, numeric data type  
**Attitude** : Global attitude towards statistics, numeric data type, likert scale 1-5  
**Deep** : multiple questions combined that explore deep learning, numeric data type, likert scale 1-5  
**Stra**: multiple questions comnined that explore strategic learning, numeric data type, likert scale 1-5  
**Surf**: multiple questions combined that explore surface learning, numeric data type, likert scale 1-5  
**Points**: points of students, numeric data type  

## 2.2.Showing graphical overview of the data
  
Installing and accessing the packages ggplot2, which is used to create graphics and GGally, an extension to ggplot2.
```
install.packages("ggplot2")
install.packages("GGally")
```
```{r inst}
library("ggplot2")

library("GGally")
```
Exploring the data by printing out a summary:
```{r sum}
summary(learning2014)
```
  Now we can get an overview about our data. There were 110 women and 56 men questioned for the data set, with an average age of about 25.5 years, the youngest beeing 17 and the oldest 55 years old. The Median of the age is 22. Most of the students are between 21 and 27 years old. The averaged attitude towards statistics is 3,14. The format ov the usual likert scale is:  
1 Strongly disagree  
2 Disagree  
3 Neither agree nor disagree  
4 Agree  
5 Strongly agree  
The mean of the strateic learning is about 3,68 the one considering strategic learning is about 3,12 and the mean of the questions considering surface learning is 2,78. The average amount of points is 22,72 with the minimum amount beeing 7 points and the maximal amount 33.

Exploring the data regarding the relationships between them:
```{r rel}
pairs(learning2014[-1], col = learning2014$gender)
```
  
  The red circles represent the male and the black the female participants and their attitude towards learning. In some of these scatter plots you can already assume a correlation. For example the number of points also seems to increase when the attitude towards statistics increases. 
A more detailed overview about the variables, distributions and relationships we can get using ggplot2 and GGally.

```{r rel2}
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```
  
  In this plot matrix I can for example see how strongly two variable are correlating. There is a correlation between attitude and points with a correlation factor of 0.451. There is little or no correlation between the points and the deep learning. I can read this from the small correlation factor of 0.01.


## 2.3 Multiple regression model
  If we want to examine the strength of the relationship between several variables (explanatory variables) on one target variable (dependet varaible) we can do this with a regression model. The model is written with the formula y ~ x1 + x2 + x3. 
 ![](/Users/eva-mariaroth/Desktop/Bildschirmfoto 2017-11-15 um 22.53.44.png)
 
 Alpha corresponds to the point where the regression line intercepts the y-axis.
Beta corresponds to the slope of the regression line. 
  The model can also be used to make predictions. It is assumed that the relationship between y and the explanatory variables is linear.
  Our target variable is "Points". As explanatory variables I chose the ones with the highest correlation coefficient considering its relation to "Points": attitude, strategic learning and surface learning.
  Hence our model looks like this:
  
```{r model}
my_model <- lm(Points ~ attitude + stra + surf, data=learning2014)
```

The summary can be printed with

```{r summodel}
summary(my_model)
```

It shows the formula that created the model and underneath a five point summary of the residuals of the model.
In the table the first column "Estimate" estimates the effect (𝝱 paramter) of the explanatory variables on the dependent variable. For attitude it is estimated with 3.3952, for strategic learning with 0,8531 and for surface learning with -0,5861. The next column gives the standard error, followed by the t-value and the p-value, that are statistical tests. A very low p-value proves a statistical relationship between the dependant and the explanatory variable. In our case a statistical relationship for "Points" with the values strategic learning and surface learning does not exist.
Hence we rewrite the model only including attitude.

```{r model2}
my_model2 <- lm(Points ~ attitude, data = learning2014)
summary(my_model2)
```

The regression line of "Points" correlating with "attitude" intercepts the y-axis at 11.63 an has a slope of 3.5.


## 2.4 Diagnostic plots

###Residuals vs Fitted values

```{r plot1}
plot(my_model2, which = 1)
```

In the linear regression model it is assumed that the errors are normally distributed and the size of an error should not dependent on the explanatory variables. This assumption can be explored with a scatter plot of residuals versus model predictions. If the points are randomly spread, without a pattern the assumtion can be proven. Here this is the case. The assumption can be proven.

###Normal QQ-plot
```{r plot2}
plot(my_model2, which = 2)
```
  
  The QQ-plot explores the assumption that the errors of the model are normally distributed.The points fall well on the line, therefor there is a reasonable fit to the normality assumtion.

###Residuals vs Leverage
```{r plot5}
plot(my_model2, which = 5)
```

Leverage measures how much impact a single observation has on the model. Plotting Residuals vs. Leverage helps to identify if single observations have a unusually high impact on the model. In our example there is not a single observation that stands out.


















